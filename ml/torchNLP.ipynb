{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ujjwal/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.36.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ujjwal/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ujjwal/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "from transformers.utils import send_example_telemetry\n",
    "from transformers import TrainingArguments, Trainer, default_data_collator\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoConfig\n",
    "from datasets import load_dataset, load_metric\n",
    "from datasets import ClassLabel, Sequence\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3508\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have no self control over food. Most people ...</td>\n",
       "      <td>I would recommend removing yourself from the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am in a high stress position for a tech comp...</td>\n",
       "      <td>Being in this position is tough. If seeking an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I need help knowing how to deal with stress. W...</td>\n",
       "      <td>Our body reacts to stress typically by breathi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My daughter didn't see her biological father f...</td>\n",
       "      <td>Hi Dillon,I'm from Canada, so I don't know the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Every winter I find myself getting sad because...</td>\n",
       "      <td>One theory is that instead of \"fighting\" your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I find myself being very outgoing most of the ...</td>\n",
       "      <td>Would you feel more secure in conversations if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>My ex-boyfriend, will not stop harassing and s...</td>\n",
       "      <td>The specific laws about this will vary from st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>I have been dating my ex-boyfriend’s cousin fo...</td>\n",
       "      <td>Does your boyfriend agree with the other peopl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>My boyfriend recently got a kitty. I hate cats...</td>\n",
       "      <td>Sorry for you and sorry for the cat because yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>My wife works late most days, so I'm lonely. I...</td>\n",
       "      <td>First things first!  Friendships that develop ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Context  \\\n",
       "0    I have no self control over food. Most people ...   \n",
       "1    I am in a high stress position for a tech comp...   \n",
       "2    I need help knowing how to deal with stress. W...   \n",
       "3    My daughter didn't see her biological father f...   \n",
       "4    Every winter I find myself getting sad because...   \n",
       "..                                                 ...   \n",
       "995  I find myself being very outgoing most of the ...   \n",
       "996  My ex-boyfriend, will not stop harassing and s...   \n",
       "997  I have been dating my ex-boyfriend’s cousin fo...   \n",
       "998  My boyfriend recently got a kitty. I hate cats...   \n",
       "999  My wife works late most days, so I'm lonely. I...   \n",
       "\n",
       "                                              Response  \n",
       "0    I would recommend removing yourself from the e...  \n",
       "1    Being in this position is tough. If seeking an...  \n",
       "2    Our body reacts to stress typically by breathi...  \n",
       "3    Hi Dillon,I'm from Canada, so I don't know the...  \n",
       "4    One theory is that instead of \"fighting\" your ...  \n",
       "..                                                 ...  \n",
       "995  Would you feel more secure in conversations if...  \n",
       "996  The specific laws about this will vary from st...  \n",
       "997  Does your boyfriend agree with the other peopl...  \n",
       "998  Sorry for you and sorry for the cat because yo...  \n",
       "999  First things first!  Friendships that develop ...  \n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Context', 'Response']\n",
      "context\n",
      "I have no self control over food. Most people stop when they've had enough, but I keep eating for the pleasure of it. Especially with sweets - I'm never done eating dessert.\n",
      "\n",
      "response\n",
      "I would recommend removing yourself from the environment you are in after you have finished eating. A simple walk around the block, calling up a friend and going to visit, or even going to another room in the house can help. If you find that you truly have no self control over food, I recommend attending Overeaters Anonymous; a 12 step group for those with compulsive issues related to food. I hope this helps.\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(\"NLPConversation.csv\")\n",
    "data=data.dropna()\n",
    "n=len(data)\n",
    "print(n)\n",
    "N=list(range(n))\n",
    "random.shuffle(N)\n",
    "data=data.iloc[N][0:1000].reset_index(drop=True)\n",
    "display(data)\n",
    "print(data.columns.tolist())\n",
    "print('context')\n",
    "print(data.iloc[0,0])\n",
    "print()\n",
    "print('response')\n",
    "print(data.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm  very depressed. How do I find someone to talk to?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Context[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "class CustomModel(AutoModelForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config.max_position_embeddings = 512  # Set your desired size\n",
    "        self.max_sequence_length = 136  # Set the same value for consistency\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n",
    "                inputs_embeds=None, start_positions=None, end_positions=None, output_attentions=None,\n",
    "                output_hidden_states=None, return_dict=None):\n",
    "\n",
    "        # Check and truncate input_ids if necessary\n",
    "        if input_ids is not None and input_ids.size(1) > self.max_sequence_length:\n",
    "            input_ids = input_ids[:, :self.max_sequence_length]\n",
    "            \n",
    "        # Print the sizes of important tensors for debugging\n",
    "        print(\"Input_ids size:\", input_ids.size())\n",
    "        print(\"Attention_mask size:\", attention_mask.size() if attention_mask is not None else None)          \n",
    "\n",
    "        # Continue with the rest of the forward method\n",
    "        return super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            start_positions=start_positions,\n",
    "            end_positions=end_positions,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "    \n",
    "model_name = \"roberta-base\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = CustomModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"roberta-base\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.36.1\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForCausalLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df(dataframe, tokenizer, max_sequence_length=512):\n",
    "    formatted_data = []\n",
    "    \n",
    "    for _, row in dataframe.iterrows():\n",
    "        input_text = f\"{row['Context']} \"\n",
    "        \n",
    "        if len(input_text) > max_sequence_length:\n",
    "            input_text = input_text[:max_sequence_length]\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=max_sequence_length, truncation=True)    \n",
    " \n",
    "        if 'Response' in row:\n",
    "            answer_text = row['Response']\n",
    "            answer_tokens = tokenizer(answer_text, return_tensors=\"pt\")[\"input_ids\"] \n",
    "        else:\n",
    "            answer_text, answer_tokens = None, None\n",
    "\n",
    "        formatted_data.append({\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "            #'labels': answer_tokens,\n",
    "        })\n",
    "\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   100,    33,   117,  1403,   797,    81,   689,     4,  1993,\n",
      "            82,   912,    77,    51,   348,    56,   615,     6,    53,    38,\n",
      "           489,  4441,    13,     5, 10483,     9,    24,     4, 17570,    19,\n",
      "         29842,   111,    38,   437,   393,   626,  4441, 17927,     4,  1437,\n",
      "             2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "formatted_df = format_df(data, tokenizer)\n",
    "print(formatted_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, formatted_data):\n",
    "        self.formatted_data = formatted_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.formatted_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.formatted_data[idx]['input_ids'].squeeze(),\n",
    "            'attention_mask': self.formatted_data[idx]['attention_mask'].squeeze(), \n",
    "            #'labels': self.formatted_data[idx]['labels'].squeeze() if self.formatted_data[idx]['labels'] is not None else None,\n",
    "            'start_positions': self.formatted_data[idx]['start_positions'] if 'start_positions' in self.formatted_data[idx] else None,\n",
    "            'end_positions': self.formatted_data[idx]['end_positions'] if 'end_positions' in self.formatted_data[idx] else None\n",
    "        }\n",
    "    \n",
    "n=len(formatted_df)\n",
    "trainset = CustomDataset(formatted_df[0:n*4//5])\n",
    "testset = CustomDataset(formatted_df[n*4//5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True)\n",
    "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], batch_first=True)\n",
    "\n",
    "    #labels = pad_sequence([item['labels'] for item in batch if item['labels'] is not None], batch_first=True) if any(item['labels'] is not None for item in batch) else None\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        #'labels': labels,\n",
    "    }\n",
    "        \n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(trainset, batch_size=batch_size, \n",
    "                              shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_dataloader = DataLoader(testset, batch_size=batch_size, \n",
    "                              shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForCausalLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    #labels = batch['labels'].to(device) if batch['labels'] is not None else None\n",
    "\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Get the predicted token IDs\n",
    "        logits = outputs.logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Convert token IDs to tokens\n",
    "        predicted_tokens = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Extend the list of predictions\n",
    "        all_predictions.extend(predicted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_269686/1592975765.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['predicted']=all_predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>Hi Attica,This is a question I think a lot of ...</td>\n",
       "      <td>My ex-boyfriend boyfriend and I lived together...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>Do the two of you ever talk over why he does t...</td>\n",
       "      <td>My boyfriend is always saying he's done with m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>Hi there, first off I have to commend you for ...</td>\n",
       "      <td>I'm a 40 year old male and having erection pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>Awareness is the first step.  Now that you kno...</td>\n",
       "      <td>I snap back and push people away. I need help ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>Thank you for reaching out! That is a great qu...</td>\n",
       "      <td>I feel like my time is going too fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Would you feel more secure in conversations if...</td>\n",
       "      <td>I find myself being very outgoing most of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>The specific laws about this will vary from st...</td>\n",
       "      <td>My ex-boyfriend, will not stop harassing and s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Does your boyfriend agree with the other peopl...</td>\n",
       "      <td>I have been dating my ex-boyfriend’s cousin fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Sorry for you and sorry for the cat because yo...</td>\n",
       "      <td>My boyfriend recently got a kitty. I hate cats...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>First things first!  Friendships that develop ...</td>\n",
       "      <td>My wife works late most days, so I'm lonely. I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Response  \\\n",
       "800  Hi Attica,This is a question I think a lot of ...   \n",
       "801  Do the two of you ever talk over why he does t...   \n",
       "802  Hi there, first off I have to commend you for ...   \n",
       "803  Awareness is the first step.  Now that you kno...   \n",
       "804  Thank you for reaching out! That is a great qu...   \n",
       "..                                                 ...   \n",
       "995  Would you feel more secure in conversations if...   \n",
       "996  The specific laws about this will vary from st...   \n",
       "997  Does your boyfriend agree with the other peopl...   \n",
       "998  Sorry for you and sorry for the cat because yo...   \n",
       "999  First things first!  Friendships that develop ...   \n",
       "\n",
       "                                             predicted  \n",
       "800  My ex-boyfriend boyfriend and I lived together...  \n",
       "801  My boyfriend is always saying he's done with m...  \n",
       "802  I'm a 40 year old male and having erection pro...  \n",
       "803  I snap back and push people away. I need help ...  \n",
       "804             I feel like my time is going too fast   \n",
       "..                                                 ...  \n",
       "995  I find myself being very outgoing most of the ...  \n",
       "996  My ex-boyfriend, will not stop harassing and s...  \n",
       "997  I have been dating my ex-boyfriend’s cousin fo...  \n",
       "998  My boyfriend recently got a kitty. I hate cats...  \n",
       "999  My wife works late most days, so I'm lonely. I...  \n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ujjwal/miniconda3/envs/torch/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ujjwal/miniconda3/envs/torch/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ujjwal/miniconda3/envs/torch/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "test=data[n*4//5:]\n",
    "test['predicted']=all_predictions\n",
    "test=test[['Response','predicted']]\n",
    "#test_df['answer_text']=test_df['answers'].apply(lambda x:x.get('text',[''])[0])\n",
    "display(test)\n",
    "\n",
    "references=test['Response'].tolist()\n",
    "hypotheses =test['predicted'].tolist()\n",
    "\n",
    "bleu_score = corpus_bleu(references, hypotheses)\n",
    "#accuracy = accuracy_score(references, hypotheses)\n",
    "#f1 = f1_score(references, hypotheses, average='micro')\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "#print(f\"Accuracy: {accuracy:.4f}\")\n",
    "#print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      "Hi Attica,This is a question I think a lot of people deal with...they feel confusion about why they can't forget about or get over (or stop connecting with) someone who they absolutely know isn't good for them. In your case, part of the problem is that he keeps trying to get back in touch with you. For some people, honestly, it's a game... to see how much power they have over you or it's their need to control you. If you don't want contact from this person, it's really important to give him clear messages about boundaries (\"Don't contact me again\"), and then ignore all of their communications. Any interactions or responses from you at all will feed their behaviour. Remind yourself why you don't want to be with them. I hear you doing that already when you say \"he has nothing to offer me\". That's great self-talk.But let's get back to the question of why we have a hard time letting go of people like this. Sometimes it's because we still hope they will change. Maybe we remember who they used to be or how they used to treat us, and we think it can go back to the way it felt in those \"good old days\". The problem there is that, over the first few years, as intimacy grows, people tend to show more of who they are, not less. So what they're eventually showing you is who they are and what they're capable of. People do change and grow, but it won't happen at your pace. Accept that person for who they are and stop expecting them to change. Another reason we can't let go is because we picture our ex with someone new in the future and we wonder if they will be a better person for the next partner in their life. This is a normal... but not a healthy... thought. It's as though we want them to stay, maybe apologize, and heal our hurts; maybe make up for past mistakes. You deserve that healing, you deserve better than you got, you deserve apologies, but to expect that from the person who hurt you and hasn't demonstrated that compassion as yet is probably foolhardy. It is my belief that underlying some of the above scenarios is the deep belief that we must somehow be responsible for the hurts our exes have caused. If you blame yourself in any way for someone's poor treatment of you, you will be dancing around trying to do things differently so that they can treat you better. It will be impossible to let go, because you blame yourself for their behaviours.Draw a line. Their behaviours are about them, not you, and the only solution is to challenge and reduce thoughts of them, and create physical and emotional distance. These things plus time will help the events of the past fade from importance.\n",
      "\n",
      "predicted\n",
      "My ex-boyfriend boyfriend and I lived together. He had a two year affair with a girl and had three pregnancies with her. One was an abortion, another was a miscarriage, and then she had the third baby. They are not together, but he continues to contact me and wants me back. He has nothing to offer me. \n"
     ]
    }
   ],
   "source": [
    "print('response')\n",
    "print(test.iloc[0,0])\n",
    "print()\n",
    "print('predicted')\n",
    "print(test.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'torchNLP.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
